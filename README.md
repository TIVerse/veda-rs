# VEDA: Versatile Execution and Dynamic Adaptation

[![Crates.io](https://img.shields.io/crates/v/veda-rs.svg)](https://crates.io/crates/veda-rs)
[![Documentation](https://docs.rs/veda-rs/badge.svg)](https://docs.rs/veda-rs)
[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue.svg)](LICENSE)
[![CI](https://github.com/veda-rs/veda/workflows/CI/badge.svg)](https://github.com/veda-rs/veda/actions)

A next-generation parallel runtime library for Rust that combines **adaptive scheduling**, **heterogeneous compute support**, and **comprehensive observability**. VEDA is designed as a complete evolution of Rayon, addressing modern application demands while maintaining zero-cost abstractions and memory safety.

## ğŸš€ Features

### Core Capabilities

- **ğŸ”„ Adaptive Thread Pools**: Dynamic worker scaling based on load and system metrics
- **âš¡ Work Stealing 2.0**: Improved algorithm with NUMA awareness and locality optimization
- **ğŸ¯ Rayon Compatibility**: Drop-in replacement for Rayon's parallel iterators
- **ğŸ”’ Scoped Parallelism**: Safe task spawning with lifetime guarantees
- **ğŸ“Š Rich Telemetry**: Per-task metrics, latency histograms, resource tracking (optional)
- **ğŸ” Deterministic Mode**: Reproducible execution for testing and debugging (optional)

### Advanced Features

- **ğŸ–¥ï¸ GPU Support**: Automatic CPU/GPU task distribution via wgpu (optional)
- **âš¡ Async Integration**: Seamless async/await support with Tokio bridge (optional)
- **ğŸ”‹ Energy-Aware Scheduling**: Power consumption and thermal throttling awareness (optional)
- **ğŸ§  NUMA Support**: NUMA-aware memory allocation and worker pinning (optional)
- **ğŸšï¸ Priority Queues**: Task prioritization with deadline scheduling
- **ğŸ›¡ï¸ Panic Isolation**: Task-level fault tolerance with recovery strategies
- **ğŸ“ Custom Allocators**: Per-thread memory pools with configurable strategies

## ğŸ“¦ Installation

Add VEDA to your `Cargo.toml`:

```toml
[dependencies]
veda-rs = "1.0"
```

### Feature Flags

```toml
[dependencies]
veda-rs = { version = "1.0", features = ["telemetry", "async", "gpu"] }
```

Available features:
- `adaptive` - Adaptive scheduling (enabled by default)
- `telemetry` - Metrics and observability (enabled by default)
- `deterministic` - Deterministic execution mode
- `async` - Async/await integration
- `gpu` - GPU compute support
- `numa` - NUMA-aware allocation
- `energy-aware` - Energy-efficient scheduling
- `custom-allocators` - Custom memory allocators

## ğŸ¯ Quick Start

### Basic Parallel Iterator

```rust
use veda::prelude::*;

fn main() {
    // Initialize the runtime
    veda::init().unwrap();
    
    // Parallel sum - identical to Rayon API
    let sum: i32 = (0..1000)
        .into_par_iter()
        .map(|x| x * 2)
        .sum();
    
    println!("Sum: {}", sum);
    
    // Cleanup
    veda::shutdown();
}
```

### Custom Configuration

```rust
use veda::{Config, SchedulingPolicy};

fn main() {
    let config = Config::builder()
        .num_threads(4)
        .scheduling_policy(SchedulingPolicy::Adaptive)
        .build()
        .unwrap();
    
    veda::init_with_config(config).unwrap();
    
    // Your parallel code here
    
    veda::shutdown();
}
```

### Scoped Parallelism

```rust
use veda::scope;

fn main() {
    veda::init().unwrap();
    
    let mut data = vec![0; 100];
    
    scope::scope(|s| {
        for chunk in data.chunks_mut(10) {
            s.spawn(move || {
                for item in chunk {
                    *item += 1;
                }
            });
        }
    }); // All spawned tasks complete here
    
    veda::shutdown();
}
```

## ğŸ“š Examples

See the [`examples/`](examples/) directory for more comprehensive examples:

- [`basic_par_iter.rs`](examples/basic_par_iter.rs) - Basic parallel iterator usage
- [`adaptive_workload.rs`](examples/adaptive_workload.rs) - Adaptive scheduling with variable workloads
- [`scoped_parallelism.rs`](examples/scoped_parallelism.rs) - Safe scoped task spawning
- [`custom_priorities.rs`](examples/custom_priorities.rs) - Task prioritization
- [`deterministic_debug.rs`](examples/deterministic_debug.rs) - Reproducible execution

Run an example:

```bash
cargo run --example basic_par_iter
```

## ğŸ”¬ Advanced Usage

### Adaptive Scheduling

VEDA automatically adjusts to workload characteristics:

```rust
let config = Config::builder()
    .scheduling_policy(SchedulingPolicy::Adaptive)
    .build()
    .unwrap();

veda::init_with_config(config).unwrap();

// Runtime adapts to variable workload automatically
let results: Vec<_> = (0..10000)
    .into_par_iter()
    .map(|i| expensive_computation(i))
    .collect();
```

### Deterministic Execution

For reproducible debugging and testing:

```rust
#[cfg(feature = "deterministic")]
{
    let config = Config::builder()
        .scheduling_policy(SchedulingPolicy::Deterministic { seed: 42 })
        .build()
        .unwrap();
    
    veda::init_with_config(config).unwrap();
    
    // Execution order is deterministic with same seed
    let result = (0..1000).into_par_iter().map(|x| x * x).sum();
}
```

### GPU Offloading

Automatic CPU/GPU task distribution (requires `gpu` feature):

```rust
#[cfg(feature = "gpu")]
{
    use veda::gpu::{GpuKernel, VectorAddKernel};
    
    let kernel = VectorAddKernel::new(1_000_000);
    let result = veda::gpu::execute(kernel).await?;
}
```

### Telemetry and Metrics

Monitor runtime performance (requires `telemetry` feature):

```rust
#[cfg(feature = "telemetry")]
{
    use veda::telemetry::export::{ConsoleExporter, MetricsExporter};
    
    // Get metrics snapshot
    let metrics = veda::telemetry::metrics::Metrics::default();
    let snapshot = metrics.snapshot();
    
    // Export to console
    let exporter = ConsoleExporter::new(true);
    exporter.export(&snapshot)?;
}
```

## ğŸ—ï¸ Architecture

VEDA is built with a modular architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VEDA Runtime                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  User Interface â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Telemetry      â”‚            â”‚
â”‚  â”‚  - par_iter()   â”‚        â”‚   Subsystem      â”‚            â”‚
â”‚  â”‚  - spawn()      â”‚        â”‚  - Metrics       â”‚            â”‚
â”‚  â”‚  - scope()      â”‚        â”‚  - Tracing       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚                          â”‚                       â”‚
â”‚           â–¼                          â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚         Adaptive Scheduler               â”‚                â”‚
â”‚  â”‚  - Load Balancing                        â”‚                â”‚
â”‚  â”‚  - Task Prioritization                   â”‚                â”‚
â”‚  â”‚  - Energy Awareness                      â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚           â”‚                                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â–¼            â–¼              â–¼              â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ CPU  â”‚   â”‚ CPU  â”‚      â”‚  GPU    â”‚    â”‚  Async  â”‚       â”‚
â”‚  â”‚ Pool â”‚   â”‚ Pool â”‚ ...  â”‚ Runtime â”‚    â”‚  Bridge â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance

VEDA aims to match or exceed Rayon's performance on uniform workloads while significantly outperforming it on variable workloads:

| Workload Type | VEDA vs Rayon | Notes |
|---------------|---------------|-------|
| CPU-bound, uniform | ~1.0x | Similar to Rayon |
| CPU-bound, variable | 1.2-1.8x | Adaptive scheduling wins |
| Small tasks (<1ms) | ~0.95x | Slight overhead from telemetry |
| Large tasks (>100ms) | 1.1-1.5x | Better load balancing |

Run benchmarks:

```bash
cargo bench
```

## ğŸ§ª Testing

VEDA includes comprehensive test coverage:

```bash
# Run unit tests
cargo test

# Run integration tests
cargo test --test integration_test

# Run stress tests (long-running)
cargo test --test stress_test -- --ignored

# Run all tests with all features
cargo test --all-features
```

## ğŸ”„ Migration from Rayon

VEDA is designed as a drop-in replacement for Rayon:

**Before (Rayon):**
```rust
use rayon::prelude::*;

fn main() {
    let sum: i32 = (0..1000).into_par_iter().sum();
}
```

**After (VEDA):**
```rust
use veda::prelude::*;

fn main() {
    veda::init().unwrap();
    let sum: i32 = (0..1000).into_par_iter().sum();
    veda::shutdown();
}
```

The main differences:
1. Explicit `init()` and `shutdown()` calls for runtime management
2. Additional configuration options available
3. Optional features for advanced capabilities

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas where we'd love help:**
- Performance optimizations
- Additional GPU kernel implementations
- Platform-specific NUMA support
- Documentation improvements
- More examples and use cases

## ğŸ“„ License

VEDA is dual-licensed under either:

- MIT License ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)
- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)

at your option.

## ğŸ™ Acknowledgments

VEDA builds upon the foundational work of:
- **Rayon** - pioneering work in Rust data parallelism
- **Tokio** - async runtime architecture
- **Crossbeam** - lock-free data structures
- **wgpu** - modern GPU abstraction

## ğŸ“ Contact and Support

- **Documentation**: https://docs.rs/veda-rs
- **Repository**: https://github.com/veda-rs/veda
- **Issues**: https://github.com/veda-rs/veda/issues
- **Discussions**: https://github.com/veda-rs/veda/discussions

## ğŸ—ºï¸ Roadmap

### Version 1.0.0 (Current)
- âœ… Adaptive thread pool with work stealing
- âœ… Rayon-compatible API surface
- âœ… Basic telemetry and metrics
- âœ… Panic isolation
- âœ… Deterministic mode
- âœ… Comprehensive test suite

### Version 1.1.0 (Planned)
- GPU compute support (wgpu integration)
- Advanced telemetry exporters
- Energy-aware scheduling
- Custom allocator support

### Version 1.2.0 (Future)
- NUMA-aware memory allocation
- Priority-based scheduling
- Real-time telemetry dashboard
- Plugin architecture for custom schedulers

---

**Made with â¤ï¸ by the VEDA Core Team**
