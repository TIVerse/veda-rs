# VEDA-RS: Versatile Execution and Dynamic Adaptation

[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://www.rust-lang.org)
[![GitHub](https://img.shields.io/github/stars/TIVerse/veda-rs?style=social)](https://github.com/TIVerse/veda-rs)

A **high-performance parallel runtime** for Rust with work-stealing and adaptive scheduling. Features 80% API compatibility with Rayon while providing enhanced capabilities for variable workloads.

## âœ¨ Features

### Core Features (Production Ready)
- âœ… **Parallel Iterators** - Range, Vec, Slice support
- âœ… **Work Stealing Scheduler** - Crossbeam-based deques
- âœ… **Scoped Parallelism** - Safe task spawning with lifetimes
- âœ… **Predicates** - `any()`, `all()`, `find_any()` with early-exit
- âœ… **Combinators** - `enumerate()`, `take()`, `skip()`, `map()`, `filter()`
- âœ… **Reductions** - `fold()`, `reduce()`, `sum()`, `collect()`
- âœ… **Panic Isolation** - Per-task panic recovery
- âœ… **Telemetry** - Metrics collection and export
- âœ… **Deterministic Mode** - Reproducible execution for debugging

### Experimental Features
- âš ï¸ **GPU Support** - Structure implemented, not yet production-ready
- âš ï¸ **Async Integration** - Basic bridge available, needs testing

## Installation

Add VEDA to your `Cargo.toml`:

```toml
[dependencies]
veda-rs = "1.0"
```

### Feature Flags

```toml
[dependencies]
veda-rs = { version = "1.0", features = ["telemetry", "async", "gpu"] }
```

Available features:
- `adaptive` - Adaptive scheduling (enabled by default)
- `telemetry` - Metrics and observability (enabled by default)
- `deterministic` - Deterministic execution mode
- `async` - Async/await integration
- `gpu` - GPU compute support
- `numa` - NUMA-aware allocation
- `energy-aware` - Energy-efficient scheduling
- `custom-allocators` - Custom memory allocators

## Quick Start

### Basic Parallel Iterator

```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    
    let sum: i32 = (0..1000)
        .into_par_iter()
        .map(|x| x * 2)
        .sum();
    
    println!("Sum: {}", sum);
    veda_rs::shutdown();
}
```

### Vec and Slice Support

```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    
    // Vec parallel processing
    let vec = vec![1, 2, 3, 4, 5];
    let doubled: Vec<i32> = vec.into_par_iter()
        .map(|x| x * 2)
        .collect();
    
    // Slice parallel processing
    let array = [1, 2, 3, 4, 5];
    let sum: i32 = array.par_iter().sum();
    
    veda_rs::shutdown();
}
```

### Predicate Methods

```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    
    // Check if any element matches
    let has_large = (0..1000).into_par_iter().any(|x| *x > 500);
    
    // Check if all elements match
    let all_positive = vec![1, 2, 3].into_par_iter().all(|x| *x > 0);
    
    // Find any matching element
    let found = (0..1000).into_par_iter().find_any(|x| *x == 42);
    
    veda_rs::shutdown();
}
```

### Custom Configuration

```rust
use veda_rs::{Config, SchedulingPolicy};

fn main() {
    let config = Config::builder()
        .num_threads(4)
        .scheduling_policy(SchedulingPolicy::Adaptive)
        .build()
        .unwrap();
    
    veda_rs::init_with_config(config).unwrap();
    veda_rs::shutdown();
}
```

### Scoped Parallelism

```rust
use veda_rs::scope;

fn main() {
    veda_rs::init().unwrap();
    
    let data = vec![1, 2, 3, 4, 5];
    
    scope::scope(|s| {
        for item in &data {
            let item = *item;
            s.spawn(move || {
                println!("Processing: {}", item * 2);
            });
        }
    });
    
    veda_rs::shutdown();
}
```

## Examples

See the [`examples/`](examples/) directory for more comprehensive examples:

- [`basic_par_iter.rs`](examples/basic_par_iter.rs) - Basic parallel iterator usage
- [`adaptive_workload.rs`](examples/adaptive_workload.rs) - Adaptive scheduling with variable workloads
- [`scoped_parallelism.rs`](examples/scoped_parallelism.rs) - Safe scoped task spawning
- [`custom_priorities.rs`](examples/custom_priorities.rs) - Task prioritization
- [`deterministic_debug.rs`](examples/deterministic_debug.rs) - Reproducible execution

Run an example:

```bash
cargo run --example basic_par_iter
```

## Advanced Usage

### Chained Operations

```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    
    let result = (0..1000).into_par_iter()
        .filter(|x| *x % 2 == 0)
        .map(|x| x * x)
        .take(10)
        .any(|x| *x > 100);
    
    println!("Result: {}", result);
    veda_rs::shutdown();
}
```

### Fold and Reduce

```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    
    // Parallel fold with reduce
    let product: i64 = (1i32..=10)
        .into_par_iter()
        .map(|x| x as i64)
        .fold(|| 1i64, |acc, x| acc * x)
        .reduce(|a, b| a * b);
    
    println!("Product: {}", product);
    veda_rs::shutdown();
}
```

### Deterministic Execution

```rust
use veda_rs::{Config, SchedulingPolicy};

fn main() {
    let config = Config::builder()
        .scheduling_policy(SchedulingPolicy::Deterministic { seed: 42 })
        .num_threads(4)
        .build()
        .unwrap();
    
    veda_rs::init_with_config(config).unwrap();
    
    // Results will be reproducible across runs
    let result: i32 = (0..1000).into_par_iter()
        .map(|x| x * x)
        .sum();
    
    veda_rs::shutdown();
}
```

### Telemetry

```rust
use veda_rs::telemetry::export::{ConsoleExporter, MetricsExporter};

fn main() {
    veda_rs::init().unwrap();
    
    // Run some parallel work
    let _: i32 = (0..10000).into_par_iter().sum();
    
    // Export metrics
    let metrics = veda_rs::telemetry::metrics::Metrics::default();
    let snapshot = metrics.snapshot();
    let exporter = ConsoleExporter::new(true);
    let _ = exporter.export(&snapshot);
    
    veda_rs::shutdown();
}
```

## Architecture

VEDA is built with a modular architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VEDA Runtime                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  User Interface â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Telemetry      â”‚            â”‚
â”‚  â”‚  - par_iter()   â”‚        â”‚   Subsystem      â”‚            â”‚
â”‚  â”‚  - spawn()      â”‚        â”‚  - Metrics       â”‚            â”‚
â”‚  â”‚  - scope()      â”‚        â”‚  - Tracing       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚                          â”‚                      â”‚
â”‚           â–¼                          â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚         Adaptive Scheduler               â”‚               â”‚
â”‚  â”‚  - Load Balancing                        â”‚               â”‚
â”‚  â”‚  - Task Prioritization                   â”‚               â”‚
â”‚  â”‚  - Energy Awareness                      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â”‚                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â–¼            â–¼              â–¼              â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ CPU  â”‚   â”‚ CPU  â”‚      â”‚  GPU    â”‚    â”‚  Async  â”‚        â”‚
â”‚  â”‚ Pool â”‚   â”‚ Pool â”‚ ...  â”‚ Runtime â”‚    â”‚  Bridge â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance

Benchmarks show similar performance to Rayon on uniform workloads, with better performance on variable workloads:

| Workload Type | VEDA vs Rayon | Notes |
|---------------|---------------|-------|
| CPU-bound, uniform | ~1.0x | Similar to Rayon |
| CPU-bound, variable | 1.2-1.8x | Adaptive scheduling wins |
| Small tasks (<1ms) | ~0.95x | Slight overhead from telemetry |
| Large tasks (>100ms) | 1.1-1.5x | Better load balancing |

Run benchmarks:

```bash
cargo bench
```

## Testing

VEDA includes comprehensive test coverage with 68 passing tests:

```bash
# Run all tests (requires serial execution due to global runtime)
cargo test -- --test-threads=1

# Run integration tests
cargo test --test integration_test -- --test-threads=1

# Run stress tests (long-running)
cargo test --test stress_test -- --test-threads=1 --ignored

# Run benchmarks
cargo bench
```

## Migration from Rayon

VEDA is designed as a drop-in replacement for Rayon:

**Before (Rayon):**
```rust
use rayon::prelude::*;

fn main() {
    let sum: i32 = (0..1000).into_par_iter().sum();
}
```

**After (VEDA):**
```rust
use veda_rs::prelude::*;

fn main() {
    veda_rs::init().unwrap();
    let sum: i32 = (0..1000).into_par_iter().sum();
    veda_rs::shutdown();
}
```

**Key Differences:**
1. Explicit `init()` and `shutdown()` calls for runtime management
2. Use `veda_rs::` instead of `rayon::`
3. Additional features: `any()`, `all()`, `find_any()`, `enumerate()`, `take()`, `skip()`
4. Vec and Slice support with `into_par_iter()` and `par_iter()`
5. Optional telemetry and deterministic execution modes

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas where we'd love help:**
- Performance optimizations
- Additional GPU kernel implementations
- Platform-specific NUMA support
- Documentation improvements
- More examples and use cases

## License

VEDA is dual-licensed under either:

- MIT License ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)
- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)

at your option.

## Acknowledgments

VEDA builds upon the foundational work of:
- **Rayon** - pioneering work in Rust data parallelism
- **Tokio** - async runtime architecture
- **Crossbeam** - lock-free data structures
- **wgpu** - modern GPU abstraction

## Contact

- **Repository**: https://github.com/TIVerse/veda-rs
- **Issues**: https://github.com/TIVerse/veda-rs/issues
- **Discussions**: https://github.com/TIVerse/veda-rs/discussions

## Implementation Status

### âœ… Completed (80% of planned features)
- âœ… Work-stealing thread pool with crossbeam
- âœ… Parallel iterators (Range, Vec, Slice)
- âœ… Core methods: map, filter, fold, reduce, sum, collect
- âœ… Predicate methods: any, all, find_any
- âœ… Combinators: enumerate, take, skip
- âœ… Scoped parallelism with lifetime safety
- âœ… Panic isolation and recovery
- âœ… Deterministic execution mode
- âœ… Basic telemetry and metrics
- âœ… Adaptive scheduling policies

### ğŸš§ In Progress (20% remaining)
- â³ Additional combinators: flat_map, zip, position
- â³ Chunking methods: par_chunks, par_windows
- â³ Try methods: try_fold, try_for_each
- â³ GPU compute activation
- â³ Async/await integration testing
- â³ NUMA optimization
